{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Value Ratio\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "train=pd.read_csv(\"lmao.csv\")\n",
    "train.isnull().sum()/len(train)*100\n",
    "\n",
    "# saving missing values in a variable \n",
    "a = train.isnull().sum()/len(train)*100 \n",
    "variables = train.columns \n",
    "variable = [ ] \n",
    "for i in range(0,12):\n",
    "    if a[i]<=20:   #setting the threshold as 20%\n",
    "        variable.append(variables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d062d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low Variance Filter\n",
    "\n",
    "train['Item_Weight'].fillna(train['Item_Weight'].median, inplace=True)\n",
    "train['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0], inplace=True)\n",
    "train.isnull().sum()/len(train)*100\n",
    "train.var()\n",
    "numeric = train[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']]\n",
    "var = numeric.var()\n",
    "numeric = numeric.columns\n",
    "variable = [ ]\n",
    "for i in range(0,len(var)):\n",
    "    if var[i]>=10:   #setting the threshold as 10%\n",
    "       variable.append(numeric[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#High Correlation filter\n",
    "\n",
    "df=train.drop('Item_Outlet_Sales', 1) \n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "df=df.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)\n",
    "model = RandomForestRegressor(random_state=1, max_depth=10)\n",
    "df=pd.get_dummies(df)\n",
    "model.fit(df,train.Item_Outlet_Sales)\n",
    "\n",
    "features = df.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-9:]  # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "feature = SelectFromModel(model)\n",
    "Fit = feature.fit_transform(df, train.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward Feature Elimination\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import datasets\n",
    "lreg = LinearRegression()\n",
    "rfe = RFE(lreg, 10)\n",
    "rfe = rfe.fit_transform(df, train.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Feature Selection\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "ffs = f_regression(df,train.Item_Outlet_Sales )\n",
    "\n",
    "variable = [ ]\n",
    "for i in range(0,len(df.columns)-1):\n",
    "    if ffs[0][i] >=10:\n",
    "       variable.append(df.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8eb46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Factor Analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "images = [cv2.imread(file) for file in glob('train/*.png')]\n",
    "\n",
    "images = np.array(images)\n",
    "images.shape\n",
    "\n",
    "image = []\n",
    "for i in range(0,60000):\n",
    "    img = images[i].flatten()\n",
    "    image.append(img)\n",
    "image = np.array(image)\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")     # Give the complete path of your train.csv file\n",
    "feat_cols = [ 'pixel'+str(i) for i in range(image.shape[1]) ]\n",
    "df = pd.DataFrame(image,columns=feat_cols)\n",
    "df['label'] = train['label']\n",
    "\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "FA = FactorAnalysis(n_components = 3).fit_transform(df[feat_cols].values)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Factor Analysis Components')\n",
    "plt.scatter(FA[:,0], FA[:,1])\n",
    "plt.scatter(FA[:,1], FA[:,2])\n",
    "plt.scatter(FA[:,2],FA[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f67934",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rndperm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mgray()\n\u001b[1;32m     11\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Principal Component Analysis (PCA)\n",
    "'''\n",
    "PCA is a technique which helps us in extracting a new set of variables from an existing large\n",
    "set of variables. These newly extracted variables are called Principal Components.'''\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "plt.gray()\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "for i in range(0,15):\n",
    "    ax = fig.add_subplot(3,5,i+1)\n",
    "    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28*3)).astype(float))\n",
    "    \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "\n",
    "plt.plot(range(4), pca.explained_variance_ratio_)\n",
    "plt.plot(range(4), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component-wise and Cumulative Explained Variance\")\n",
    "\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(12, 8))\n",
    "sns.heatmap(pca.components_[0, :].reshape(28, 84), ax=axarr[0][0], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[1, :].reshape(28, 84), ax=axarr[0][1], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[2, :].reshape(28, 84), ax=axarr[1][0], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[3, :].reshape(28, 84), ax=axarr[1][1], cmap='gray_r')\n",
    "axarr[0][0].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[0]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[0][1].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[1]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[1][0].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[2]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[1][1].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[3]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[0][0].set_aspect('equal')\n",
    "axarr[0][1].set_aspect('equal')\n",
    "axarr[1][0].set_aspect('equal')\n",
    "axarr[1][1].set_aspect('equal')\n",
    "\n",
    "plt.suptitle('4-Component PCA')\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "svd = TruncatedSVD(n_components=3, random_state=42).fit_transform(df[feat_cols].values)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('SVD Components')\n",
    "plt.scatter(svd[:,0], svd[:,1])\n",
    "plt.scatter(svd[:,1], svd[:,2])\n",
    "plt.scatter(svd[:,2],svd[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent Component Analysis\n",
    "'''\n",
    "This algorithm assumes that the given variables are linear mixtures of some unknown latent \n",
    "variables. It also assumes that these latent variables are mutually independent'''\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA \n",
    "ICA = FastICA(n_components=3, random_state=12) \n",
    "X=ICA.fit_transform(df[feat_cols].values)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('ICA Components')\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(X[:,1], X[:,2])\n",
    "plt.scatter(X[:,2], X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ad42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods Based on Projections\n",
    "'''\n",
    "By projecting one vector onto the other, dimensionality can be reduced.\n",
    "\n",
    "In projection techniques, multi-dimensional data is represented by projecting its points onto \n",
    "a lower-dimensional space.'''\n",
    "\n",
    "\n",
    "from sklearn import manifold \n",
    "trans_data = manifold.Isomap(n_neighbors=5, n_components=3, n_jobs=-1).fit_transform(df[feat_cols][:6000].values)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Decomposition using ISOMAP')\n",
    "plt.scatter(trans_data[:,0], trans_data[:,1])\n",
    "plt.scatter(trans_data[:,1], trans_data[:,2])\n",
    "plt.scatter(trans_data[:,2], trans_data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20246152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t- Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "'''\n",
    "Local approaches : They maps nearby points on the manifold to nearby points in the low \n",
    "dimensional representation.\n",
    "\n",
    "Global approaches : They attempt to preserve geometry at all scales, i.e. mapping nearby points \n",
    "on manifold to nearby points in low dimensional representation as well as far away points to far \n",
    "away points.\n",
    "\n",
    "t-SNE is one of the few algorithms which is capable of retaining both local and global structure\n",
    "of the data at the same time.It calculates the probability similarity of points in high \n",
    "dimensional space as well as in low dimensional space'''\n",
    "\n",
    "from sklearn.manifold import TSNE \n",
    "tsne = TSNE(n_components=3, n_iter=300).fit_transform(df[feat_cols][:6000].values)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('t-SNE components')\n",
    "plt.scatter(tsne[:,0], tsne[:,1])\n",
    "plt.scatter(tsne[:,1], tsne[:,2])\n",
    "plt.scatter(tsne[:,2], tsne[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce982bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP\n",
    "'''\n",
    "t-SNE works very well on large datasets but it also has it’s limitations, such as loss of \n",
    "large-scale information, slow computation time, and inability to meaningfully represent very \n",
    "large datasets. Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction \n",
    "technique that can preserve as much of the local, and more of the global data structure as \n",
    "compared to t-SNE, with a shorter runtime.'''\n",
    "\n",
    "\n",
    "import umap\n",
    "umap_data = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=3).fit_transform(df[feat_cols][:6000].values)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Decomposition using UMAP')\n",
    "plt.scatter(umap_data[:,0], umap_data[:,1])\n",
    "plt.scatter(umap_data[:,1], umap_data[:,2])\n",
    "plt.scatter(umap_data[:,2], umap_data[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
